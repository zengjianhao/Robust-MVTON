<!doctype html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="./output.css" rel="stylesheet">
</head>
<body class="mx-auto max-w-7xl">
    <h1 class="text-center text-4xl font-semibold tracking-tight p-10">
        Robust-MVTON: Learning Cross-Pose Feature Alignment and Fusion for Robust Multi-View Virtual Try-On
    </h1>

    <!-- 作者 -->
    <p class="text-center text-xl font-normal leading-8 p-1">
        <a href="https://scholar.google.com/citations?user=sshKuUMAAAAJ&hl=zh-CN" target="_blank" class="text-cyan-600">Nannan Zhang</a><span class="align-super text-xs">1*</span>,
        <a href="https://williamium3000.github.io/" target="_blank" class="text-cyan-600">Yijiang Li</a><span class="align-super text-xs">3*</span>,
        <a href="https://dongdu3.github.io/" target="_blank" class="text-cyan-600">Dong Du</a><span class="align-super text-xs">2†</span>,
        <a href="https://github.com/Zheng-Chong" target="_blank" class="text-cyan-600">Zheng Chong</a><span class="align-super text-xs">4</span>,
        <a href="https://taited.github.io/" target="_blank" class="text-cyan-600">Zhengwentai Sun</a><span class="align-super text-xs">1</span>,
        <a href="https://zengjianhao.github.io/" target="_blank" class="text-cyan-600">Jianhao Zeng</a><span class="align-super text-xs">5</span>,
        <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=tvjQ7GUAAAAJ" target="_blank" class="text-cyan-600">Yusheng Dai</a><span class="align-super text-xs">6</span>,
    </p>
    <p class="text-center text-xl font-normal leading-8 p-1">
        <a href="https://xiezhy6.github.io/" target="_blank" class="text-cyan-600">Zhenyu Xie</a><span class="align-super text-xs">4</span>,
        Hairui Zhu<span class="align-super text-xs">1</span>,
        <a href="https://gaplab.cuhk.edu.cn/pages/people" target="_blank" class="text-cyan-600">Xiaoguang Han</a><span class="align-super text-xs">1†</span>
    </p>

    <!-- 单位 -->
    <p class="text-center text-xl font-normal leading-8 p-1">
        <span class="align-super text-xs">1</span>CUHKSZ &emsp;
        <span class="align-super text-xs">2</span>NJUST &emsp;
        <span class="align-super text-xs">3</span>UCSD &emsp;
        <span class="align-super text-xs">4</span>SYSU &emsp;
        <span class="align-super text-xs">5</span>Westlake University &emsp;
        <span class="align-super text-xs">6</span>USTC &emsp;
    </p>


    <!-- 邮箱 -->
    <p class="text-center p-2">nannanzhang@link.cuhk.edu.cn</p>

    <!-- 链接 -->
    <div class="mx-auto flex flex-row items-start justify-center gap-2 p-4">
        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Robust-MVTON_Learning_Cross-Pose_Feature_Alignment_and_Fusion_for_Robust_Multi-View_CVPR_2025_paper.pdf" target="_blank" class="inline-flex items-center px-4 py-2 rounded-full bg-black text-white">
            <span class="mr-2">
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-file-earmark-pdf-fill" viewBox="0 0 16 16">
                <path d="M5.523 12.424q.21-.124.459-.238a8 8 0 0 1-.45.606c-.28.337-.498.516-.635.572l-.035.012a.3.3 0 0 1-.026-.044c-.056-.11-.054-.216.04-.36.106-.165.319-.354.647-.548m2.455-1.647q-.178.037-.356.078a21 21 0 0 0 .5-1.05 12 12 0 0 0 .51.858q-.326.048-.654.114m2.525.939a4 4 0 0 1-.435-.41q.344.007.612.054c.317.057.466.147.518.209a.1.1 0 0 1 .026.064.44.44 0 0 1-.06.2.3.3 0 0 1-.094.124.1.1 0 0 1-.069.015c-.09-.003-.258-.066-.498-.256M8.278 6.97c-.04.244-.108.524-.2.829a5 5 0 0 1-.089-.346c-.076-.353-.087-.63-.046-.822.038-.177.11-.248.196-.283a.5.5 0 0 1 .145-.04c.013.03.028.092.032.198q.008.183-.038.465z"/>
                <path fill-rule="evenodd" d="M4 0h5.293A1 1 0 0 1 10 .293L13.707 4a1 1 0 0 1 .293.707V14a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2m5.5 1.5v2a1 1 0 0 0 1 1h2zM4.165 13.668c.09.18.23.343.438.419.207.075.412.04.58-.03.318-.13.635-.436.926-.786.333-.401.683-.927 1.021-1.51a11.7 11.7 0 0 1 1.997-.406c.3.383.61.713.91.95.28.22.603.403.934.417a.86.86 0 0 0 .51-.138c.155-.101.27-.247.354-.416.09-.181.145-.37.138-.563a.84.84 0 0 0-.2-.518c-.226-.27-.596-.4-.96-.465a5.8 5.8 0 0 0-1.335-.05 11 11 0 0 1-.98-1.686c.25-.66.437-1.284.52-1.794.036-.218.055-.426.048-.614a1.24 1.24 0 0 0-.127-.538.7.7 0 0 0-.477-.365c-.202-.043-.41 0-.601.077-.377.15-.576.47-.651.823-.073.34-.04.736.046 1.136.088.406.238.848.43 1.295a20 20 0 0 1-1.062 2.227 7.7 7.7 0 0 0-1.482.645c-.37.22-.699.48-.897.787-.21.326-.275.714-.08 1.103"/>
                </svg>
            </span>
            <span>Paper</span>
        </a>
        <a href="#" target="_blank" class="inline-flex items-center px-4 py-2 rounded-full bg-black text-white">
            <span class="mr-2">
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
                <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8"/>
                </svg>
            </span>
            <span>Code</span>
        </a>
    </div>

    <!-- Teaser -->
    <img src="./images/Teaser.png" class="max-w-5xl mx-auto mt-8">


    <!-- Abstract -->
    <h2 class="text-center text-4xl font-semibold tracking-tight p-5 mt-5">Abstract</h2>
    <p class="mx-auto max-w-5xl text-justify text-lg">
        This paper tackles the emerging challenge of multi-view virtual try-on, utilizing both front- and back-view clothing images as inputs. Extending frontal try-on methods to a multi-view context is not straightforward. Simply concatenating the two input views or encoding their features for a generative model, such as a diffusion model, often fails to produce satisfactory results. The main challenge lies in effectively extracting and fusing meaningful clothing features from these input views. Existing explicit warpingbased methods, which establish direct correspondence between input and target views, tend to introduce artifacts, particularly when there is a significant disparity between the input and target views. Conversely, implicit encodingbased methods often lose spatial information about clothing, resulting in outputs that lack detail. To overcome these challenges, we propose Robust-MVTON, an end-toend method for robust and high-quality multi-view try-ons. Our approach introduces a novel cross-pose feature alignment technique to guide the fusion of clothing features and incorporates a newly designed loss function for training. With the fused multi-scale clothing features, we employ a coarse-to-fine diffusion model to generate realistic and detailed results. Extensive experiments conducted on the Deepfashion and MPV datasets affirm the superiority of our method, achieving state-of-the-art performance.
    </p>


    <!-- Pipeline -->
    <div class="max-w-5xl mx-auto">
        <h2 class="text-3xl font-semibold tracking-tight p-4 mt-6">Robust-MVTON Pipeline</h2>
        <img src="./images/Frame.png" class="mx-auto">
    </div>




    <!-- Comparison result -->
    <div class="max-w-5xl mx-auto">
        <h2 class="text-3xl font-semibold tracking-tight p-4 mt-6">Comparison Results</h2>
        <div class="grid grid-cols-1 gap-4">
            <img src="./images/comp3.png" class="">
            <img src="./images/comp1.png" class="">
            <img src="./images/comp2.png" class="">

        </div>
    </div>



    <!-- Results -->
    <div class="max-w-5xl mx-auto">
        <h2 class="text-3xl font-semibold tracking-tight p-4 mt-6">Results Gallery</h2>
        <div class="grid grid-cols-1 gap-4">
            <img src="./images/Our_Result1.png" class="">
            <img src="./images/Our_Result2.png" class="">
        </div>
    </div>






    <!-- In the Wild -->
    <div class="max-w-5xl mx-auto">
        <h2 class="text-3xl font-semibold tracking-tight p-4 mt-6">In the Wild</h2>
        <div class="grid grid-cols-1 gap-4">
            <img src="./images/in_the_wild3.png" class="">
            <img src="./images/in_the_wild2.png" class="">
            <img src="./images/in_the_wild1.png" class="">
        </div>
    </div>





    <!-- BibTeX -->
    <div class="max-w-5xl mx-auto">
    <h2 class="text-3xl font-semibold tracking-tight p-4 mt-6">BibTeX</h2>
        <div class="mx-auto max-w-5xl">
        <pre class="bg-gray-100 text-black p-4 rounded-lg overflow-x-auto">
            <code class="font-mono">
    @inproceedings{zhang2025robust,
        title={Robust-MVTON: Learning Cross-Pose Feature Alignment and Fusion for Robust Multi-View Virtual Try-On},
        author={Zhang, Nannan and Li, Yijiang and Du, Dong and Chong, Zheng and Sun, Zhengwentai and Zeng, Jianhao and Dai, Yusheng and Xie, Zhengyu and Zhu, Hairui and Han, Xiaoguang},
        booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
        pages={16029--16039},
        year={2025}
    }
            </code>
        </pre>

    </div>




    <div class="mb-16"></div>

</body>
</html>